{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSYE7245 - Big Data Systems & Intelligent Analytics (Spring 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Abnormal Event Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kirti Akhilesh Shukla - NUID 001448312"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Anomaly detection in videos, Instead of treating it as supervised learning and Labeling the videos as Normal and abnormal, I have used another approach and did the anomaly detection by unsupervised learning.It is difficult to obtain abnormal videos as compared to normal videos. Even in self automated cars, the biggest challenge is to get accident videos because it is very difficult to get them and also to generate them.\n",
    "Spatiotemporal architecture is proposed for anomaly detection in videos including crowded scenes. It contains two main components, one for spatial feature representation, and one for learning the temporal evolution of the spatial features.\n",
    "I have trained my model by only normal videos(some outliers) and while testing, when abnormal videos are given to this trained model, the reconstruction error for such videos goes above threshold and anomaly is detected.\n",
    "\n",
    "This application can be used in video surveillance to detect abnormal events as itâ€™s based on unsupervised learning, the advantage being that the only ingredient required is a long video segment containing only normal events in a fixed view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suspicious events that are of interest in long video sequences, such as surveillance footage usually have an extremely low probability of occurring. Manually detecting such events, or anomalies, is a very meticulous job that often requires more manpower than is generally available. Hence, there is a need for Automate detection.\n",
    "\n",
    " \n",
    "\n",
    "Treating the task as a binary classification problem (normal and abnormal)  proved it being effective and accurate, but the practicality of such method is limited since footages of abnormal events are difficult to obtain due to its rarity. Hence, it is more efficient to train a model using little to no supervision, including spatiotemporal features and autoencoders . Unlike supervised methods, these methods only require unlabelled video footages which contain little or no abnormal event, which are easy to obtain in real-world applications.\n",
    "\n",
    " In this project, video data set is represented by a set of general features, which are inferred automatically from a long video footage through a deep learning approach. Specifically, a deep neural network composed of a stack of convolutional autoencoders was used to process video frames in an unsupervised manner that captured spatial structures in the data, which, grouped together, compose the video representation. Then, this representation is fed into a stack of convolutional temporal autoencoders to learn the regular temporal patterns.\n",
    "\n",
    "The method described here is based on the principle that when an abnormal event occurs, the most recent frames of video will be significantly different than the older frames. Trained an end-to-end model that consists of a spatial feature extractor and a temporal encoder-decoder which together learns the temporal patterns of the input volume of frames. The model is trained with video volumes consists of only normal scenes, with the objective to minimize the reconstruction error between the input video volume and the output video volume reconstructed by the learned model. After the model is properly trained, normal video volume is expected to have low reconstruction error, whereas video volume consisting of abnormal scenes is expected to have high reconstruction error. By thresholding on the error produced by each testing input volumes, our system will be able to detect when an abnormal event occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.image import img_to_array,load_img\n",
    "from keras.layers import Conv3D,ConvLSTM2D,Conv3DTranspose,PReLU,BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import os \n",
    "from scipy.misc import imresize \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Videos\n",
    "The training and the testing videos are loaded. The approach used to load is by extracting frames from the video. This can be done using a linux command __ffmpeg -i {video filename} -r {fps} {image filename}.__ These images are then converted to an array and then to grayscale and saved in a list. The frames are then resized to **227x227** size and then normalized before passing to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store(image_path,imagestore):\n",
    "    #Loading the image frames using keras load_img.\n",
    "    img=load_img(image_path)\n",
    "    #Converting the loaded image to array using keras img_to_array.\n",
    "    img=img_to_array(img)\n",
    "    #Resize the Image to (227,227,3) for the model to be able to process it. \n",
    "    img=imresize(img,(227,227,3))\n",
    "    #Convert the Image to Grayscale (Code referred from stackoverflow post mentioned in citations).\n",
    "    gray=0.2989*img[:,:,0]+0.5870*img[:,:,1]+0.1140*img[:,:,2]\n",
    "    #Appending each image to a list of all image frames.\n",
    "    imagestore.append(gray)\n",
    "    \n",
    "    \n",
    "def preprocess(video_source_path, imagestore, outputName, fps):\n",
    "    #List of all Videos in the Source Directory.\n",
    "    videos=os.listdir(video_source_path)\n",
    "    #Make a temp dir to store all the frames\n",
    "    if not os.path.isdir(video_source_path+'/frames'):\n",
    "        os.mkdir(video_source_path+'/frames')\n",
    "    framepath=video_source_path+'/frames'\n",
    "    for video in videos:\n",
    "        if not video == 'frames':\n",
    "            #Extracts frames from the video. The number after -r is the number of fps extracted.\n",
    "            os.system( 'ffmpeg -i {}/{} -r {}  {}/frames/%05d.jpg'.format(video_source_path,video,fps,video_source_path))\n",
    "            images=os.listdir(framepath)\n",
    "            for image in images:\n",
    "                image_path=framepath+ '/'+ image\n",
    "                #Store the image in the aggregated image list\n",
    "                store(image_path,imagestore)\n",
    "            os.system('rm -r {}/*'.format(framepath))\n",
    "    imagestore=np.array(imagestore)\n",
    "    a,b,c=imagestore.shape\n",
    "    #Reshape to (227,227,batch_size). This is done so that 10 frames can be used as a bunch size for training ahead.\n",
    "    print(imagestore.shape)\n",
    "    imagestore.resize(b,c,a)\n",
    "    print(imagestore.shape)\n",
    "    #Normalize the image\n",
    "    imagestore=(imagestore-imagestore.mean())/(imagestore.std())\n",
    "    #Clip negative Values. Negative values are clipped to 0 and values > 1 are clipped to 1. This is done to restrict\n",
    "    #the range of values between 0 and 1.\n",
    "    imagestore=np.clip(imagestore,0,1)\n",
    "    #Save all the images in a numpy array file.\n",
    "    np.save(outputName+'.npy',imagestore)\n",
    "    #Remove Buffer Directory\n",
    "    os.system('rm -r {}'.format(framepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Both the training and the testing videos are preprocessed into frames and then stored in a file as an numpy array object.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1251, 227, 227)\n",
      "(227, 227, 1251)\n",
      "(1258, 227, 227)\n",
      "(227, 227, 1258)\n"
     ]
    }
   ],
   "source": [
    "source_path= os.getcwd()+'/data/AvenueDataset/training_videos/'\n",
    "target_path= os.getcwd()+'/data/AvenueDataset/testing_videos/'\n",
    "fps=2\n",
    "imstore=[]\n",
    "preprocess(source_path, imstore, 'training', fps)\n",
    "del imstore\n",
    "imstore = []\n",
    "preprocess(target_path, imstore, 'testing', fps)\n",
    "del imstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building\n",
    "The input to the model is video volumes, where each volume consists of 10 consecutive frames with various skipping strides.  As the number of parameters\n",
    "in this model is large, large amount of training data is needed. We perform data augmentation in the temporal dimension to increase\n",
    "the size of the training dataset. To generate these volumes, we concatenate\n",
    "frames with stride-1, stride-2, and stride-3. For example, the first stride-1 sequence is made up of frame {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, whereas the first\n",
    "stride-2 sequence contains frame number {1, 3, 5, 7, 9, 11, 13, 15, 17, 19}, and\n",
    "stride-3 sequence would contain frame number {1, 4, 7, 10, 13, 16, 19, 22, 25,\n",
    "28}. Now the input is ready for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The model architecture consists of two parts â€” spatial autoencoder for learning spatial structures of each video frame, and\n",
    "temporal encoder-decoder for learning temporal patterns of the encoded spatial structures.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spatial encoder and decoder\n",
    "have two convolutional and deconvolutional layers respectively, while the temporal encoder is a three-layer convolutional long short term memory (LSTM)\n",
    "model. Convolutional layers are well-known for its superb performance in object recognition, while LSTM model is widely used for sequence learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders, as the name suggests, consist of two stages: encoding and decoding. It was first used to reduce dimensionality by setting the number of\n",
    "encoder output units less than the input. The model is usually trained using\n",
    "back-propagation in an unsupervised manner, by minimizing the reconstruction\n",
    "error of the decoding results from the original inputs. With the activation function chosen to be nonlinear, an autoencoder can extract more useful features\n",
    "than some common linear transformation methods such as PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_3 (Conv3D)            (None, 75, 75, 10, 256)   6656      \n",
      "_________________________________________________________________\n",
      "p_re_lu_5 (PReLU)            (None, 75, 75, 10, 256)   14400000  \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 75, 75, 10, 256)   1024      \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 36, 36, 10, 128)   819328    \n",
      "_________________________________________________________________\n",
      "p_re_lu_6 (PReLU)            (None, 36, 36, 10, 128)   1658880   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 36, 36, 10, 128)   512       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_4 (ConvLSTM2D)  (None, 36, 36, 10, 128)   1180160   \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_5 (ConvLSTM2D)  (None, 36, 36, 10, 64)    442624    \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_6 (ConvLSTM2D)  (None, 36, 36, 10, 128)   885248    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 36, 36, 10, 128)   512       \n",
      "_________________________________________________________________\n",
      "p_re_lu_7 (PReLU)            (None, 36, 36, 10, 128)   1658880   \n",
      "_________________________________________________________________\n",
      "conv3d_transpose_3 (Conv3DTr (None, 75, 75, 10, 256)   819456    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 75, 75, 10, 256)   1024      \n",
      "_________________________________________________________________\n",
      "p_re_lu_8 (PReLU)            (None, 75, 75, 10, 256)   14400000  \n",
      "_________________________________________________________________\n",
      "conv3d_transpose_4 (Conv3DTr (None, 227, 227, 10, 1)   6401      \n",
      "=================================================================\n",
      "Total params: 36,280,705\n",
      "Trainable params: 36,279,169\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def loadModel():\n",
    "    model=Sequential()\n",
    "    \n",
    "    model.add(Conv3D(filters=256,kernel_size=(5,5,1),strides=(3,3,1),padding='valid',input_shape=(227,227,10,1)))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv3D(filters=128,kernel_size=(5,5,1),strides=(2,2,1),padding='valid'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(ConvLSTM2D(filters=128,kernel_size=(3,3),strides=1,padding='same',dropout=0.4,recurrent_dropout=0.3,return_sequences=True))\n",
    "    model.add(ConvLSTM2D(filters=64,kernel_size=(3,3),strides=1,padding='same',dropout=0.3,return_sequences=True))\n",
    "    model.add(ConvLSTM2D(filters=128,kernel_size=(3,3),strides=1,return_sequences=True, padding='same',dropout=0.5))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(PReLU())\n",
    "    model.add(Conv3DTranspose(filters=256,kernel_size=(5,5,1),strides=(2,2,1),padding='valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(PReLU())\n",
    "    model.add(Conv3DTranspose(filters=1,kernel_size=(5,5,1),strides=(3,3,1),padding='valid'))\n",
    "    return model\n",
    "\n",
    "model = loadModel()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10 10 frames are passed to the model at once so that the model can find features in the sequence as described earlier.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFrames(fileName):\n",
    "    #Loads a stored numpy array file.\n",
    "    X_train=np.load(fileName)\n",
    "    frames=X_train.shape[2]\n",
    "    print(frames)\n",
    "    #Need to make number of batch_size(frames) divisible by 10\n",
    "    frames=frames-frames%10\n",
    "    #Removing the remainder frames.\n",
    "    X_train=X_train[:,:,:frames]\n",
    "    #Reshaping the training images in such a way that if there were total 1251 frames extracted,\n",
    "    #the last 1 frame are deleted and from the remaining 1250 frames, divided in bunches of 10 consecutive frames at once.\n",
    "    #So now 125 bunches of frames are trained where each bunch has 10 consecutive images of size 227x227.\n",
    "    X_train=X_train.reshape(-1,227,227,10)\n",
    "    print(X_train.shape)\n",
    "    X_train=np.expand_dims(X_train,axis=4)\n",
    "    print(X_train.shape)\n",
    "    #Since it is unsupervised learning, x_train and y_train will be same.\n",
    "    Y_train=X_train.copy()\n",
    "    return X_train, Y_train\n",
    "\n",
    "#Simple plot to visualize the model's training history\n",
    "def visualizeModel(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title('model history')\n",
    "    plt.ylabel('accuracy & loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['Accuracy', 'Loss'], loc='best')\n",
    "    plt.show()\n",
    "``\n",
    "def train(model,filename):\n",
    "    X_train, Y_train = loadFrames(filename+'.npy')\n",
    "    epochs=125\n",
    "    batch_size=5\n",
    "    model.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy'])\n",
    "    history = model.fit(X_train,Y_train,batch_size=batch_size,epochs=125)\n",
    "    model.save('model_train_new.h5')\n",
    "    visualizeModel(history)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1251\n",
      "(125, 227, 227, 10)\n",
      "(125, 227, 227, 10, 1)\n",
      "Epoch 1/125\n",
      "125/125 [==============================] - 40s 322ms/step - loss: 0.2372 - acc: 0.5295\n",
      "Epoch 2/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0859 - acc: 0.7233\n",
      "Epoch 3/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0745 - acc: 0.7333\n",
      "Epoch 4/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0708 - acc: 0.7384\n",
      "Epoch 5/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0692 - acc: 0.7397\n",
      "Epoch 6/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0684 - acc: 0.7402\n",
      "Epoch 7/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0662 - acc: 0.7408\n",
      "Epoch 8/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0615 - acc: 0.7442\n",
      "Epoch 9/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0574 - acc: 0.7488\n",
      "Epoch 10/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0515 - acc: 0.7528\n",
      "Epoch 11/125\n",
      "125/125 [==============================] - 17s 135ms/step - loss: 0.0445 - acc: 0.7595\n",
      "Epoch 12/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0376 - acc: 0.7693\n",
      "Epoch 13/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0332 - acc: 0.7745\n",
      "Epoch 14/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0303 - acc: 0.7772\n",
      "Epoch 15/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0284 - acc: 0.7788\n",
      "Epoch 16/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0268 - acc: 0.7800\n",
      "Epoch 17/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0256 - acc: 0.7810\n",
      "Epoch 18/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0246 - acc: 0.7818\n",
      "Epoch 19/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0237 - acc: 0.7825\n",
      "Epoch 20/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0229 - acc: 0.7832\n",
      "Epoch 21/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0222 - acc: 0.7838\n",
      "Epoch 22/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0216 - acc: 0.7843\n",
      "Epoch 23/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0212 - acc: 0.7847\n",
      "Epoch 24/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0207 - acc: 0.7851\n",
      "Epoch 25/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0203 - acc: 0.7855\n",
      "Epoch 26/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0206 - acc: 0.7856\n",
      "Epoch 27/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0195 - acc: 0.7862\n",
      "Epoch 28/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0191 - acc: 0.7865\n",
      "Epoch 29/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0186 - acc: 0.7868\n",
      "Epoch 30/125\n",
      "125/125 [==============================] - 17s 138ms/step - loss: 0.0185 - acc: 0.7869\n",
      "Epoch 31/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0182 - acc: 0.7872\n",
      "Epoch 32/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0180 - acc: 0.7874\n",
      "Epoch 33/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0175 - acc: 0.7876\n",
      "Epoch 34/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0172 - acc: 0.7878\n",
      "Epoch 35/125\n",
      "125/125 [==============================] - 17s 135ms/step - loss: 0.0173 - acc: 0.7879\n",
      "Epoch 36/125\n",
      "125/125 [==============================] - 17s 135ms/step - loss: 0.0171 - acc: 0.7880\n",
      "Epoch 37/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0166 - acc: 0.7882\n",
      "Epoch 38/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0166 - acc: 0.7883\n",
      "Epoch 39/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0164 - acc: 0.7884\n",
      "Epoch 40/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0162 - acc: 0.7886\n",
      "Epoch 41/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0160 - acc: 0.7887\n",
      "Epoch 42/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0157 - acc: 0.7888\n",
      "Epoch 43/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0155 - acc: 0.7889\n",
      "Epoch 44/125\n",
      "125/125 [==============================] - 17s 135ms/step - loss: 0.0155 - acc: 0.7890\n",
      "Epoch 45/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0154 - acc: 0.7891\n",
      "Epoch 46/125\n",
      "125/125 [==============================] - 17s 135ms/step - loss: 0.0150 - acc: 0.7893\n",
      "Epoch 47/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0150 - acc: 0.7893\n",
      "Epoch 48/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0150 - acc: 0.7894\n",
      "Epoch 49/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0149 - acc: 0.7895\n",
      "Epoch 50/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0147 - acc: 0.7896\n",
      "Epoch 51/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0144 - acc: 0.7897\n",
      "Epoch 52/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0143 - acc: 0.7898\n",
      "Epoch 53/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0141 - acc: 0.7899\n",
      "Epoch 54/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0140 - acc: 0.7900\n",
      "Epoch 55/125\n",
      "125/125 [==============================] - 17s 135ms/step - loss: 0.0141 - acc: 0.7900\n",
      "Epoch 56/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0137 - acc: 0.7902\n",
      "Epoch 57/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0141 - acc: 0.7901\n",
      "Epoch 58/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0137 - acc: 0.7902\n",
      "Epoch 59/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0135 - acc: 0.7903\n",
      "Epoch 60/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0133 - acc: 0.7904\n",
      "Epoch 61/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0134 - acc: 0.7905\n",
      "Epoch 62/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0137 - acc: 0.7905\n",
      "Epoch 63/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0131 - acc: 0.7906\n",
      "Epoch 64/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0131 - acc: 0.7907\n",
      "Epoch 65/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0129 - acc: 0.7908\n",
      "Epoch 66/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0126 - acc: 0.7909\n",
      "Epoch 67/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0128 - acc: 0.7909\n",
      "Epoch 68/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0126 - acc: 0.7910\n",
      "Epoch 69/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0126 - acc: 0.7910\n",
      "Epoch 70/125\n",
      "125/125 [==============================] - 17s 138ms/step - loss: 0.0124 - acc: 0.7910\n",
      "Epoch 71/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0127 - acc: 0.7911\n",
      "Epoch 72/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0124 - acc: 0.7912\n",
      "Epoch 73/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0123 - acc: 0.7912\n",
      "Epoch 74/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0123 - acc: 0.7913\n",
      "Epoch 75/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0122 - acc: 0.7913\n",
      "Epoch 76/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0122 - acc: 0.7914\n",
      "Epoch 77/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0119 - acc: 0.7915\n",
      "Epoch 78/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0121 - acc: 0.7915\n",
      "Epoch 79/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0120 - acc: 0.7915\n",
      "Epoch 80/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0117 - acc: 0.7916\n",
      "Epoch 81/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0114 - acc: 0.7917\n",
      "Epoch 82/125\n",
      "125/125 [==============================] - 17s 135ms/step - loss: 0.0115 - acc: 0.7917\n",
      "Epoch 83/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0116 - acc: 0.7917\n",
      "Epoch 84/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0113 - acc: 0.7918\n",
      "Epoch 85/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0112 - acc: 0.7919\n",
      "Epoch 86/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0114 - acc: 0.7919\n",
      "Epoch 87/125\n",
      "125/125 [==============================] - 17s 138ms/step - loss: 0.0116 - acc: 0.7919\n",
      "Epoch 88/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0111 - acc: 0.7920\n",
      "Epoch 89/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0112 - acc: 0.7920\n",
      "Epoch 90/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0110 - acc: 0.7921\n",
      "Epoch 91/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0108 - acc: 0.7921\n",
      "Epoch 92/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0108 - acc: 0.7922\n",
      "Epoch 93/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0110 - acc: 0.7922\n",
      "Epoch 94/125\n",
      "125/125 [==============================] - 17s 138ms/step - loss: 0.0108 - acc: 0.7922\n",
      "Epoch 95/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0107 - acc: 0.7923\n",
      "Epoch 96/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0108 - acc: 0.7922\n",
      "Epoch 97/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0107 - acc: 0.7923\n",
      "Epoch 98/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0105 - acc: 0.7924\n",
      "Epoch 99/125\n",
      "125/125 [==============================] - 17s 138ms/step - loss: 0.0105 - acc: 0.7924\n",
      "Epoch 100/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0105 - acc: 0.7924\n",
      "Epoch 101/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0105 - acc: 0.7924\n",
      "Epoch 102/125\n",
      "125/125 [==============================] - 17s 138ms/step - loss: 0.0106 - acc: 0.7925\n",
      "Epoch 103/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0104 - acc: 0.7925\n",
      "Epoch 104/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0104 - acc: 0.7925\n",
      "Epoch 105/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0103 - acc: 0.7926\n",
      "Epoch 106/125\n",
      "125/125 [==============================] - 17s 138ms/step - loss: 0.0102 - acc: 0.7926\n",
      "Epoch 107/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0102 - acc: 0.7926\n",
      "Epoch 108/125\n",
      "125/125 [==============================] - 17s 138ms/step - loss: 0.0101 - acc: 0.7926\n",
      "Epoch 109/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0101 - acc: 0.7927\n",
      "Epoch 110/125\n",
      "125/125 [==============================] - 17s 138ms/step - loss: 0.0100 - acc: 0.7927\n",
      "Epoch 111/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0102 - acc: 0.7927\n",
      "Epoch 112/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0101 - acc: 0.7927\n",
      "Epoch 113/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0100 - acc: 0.7928\n",
      "Epoch 114/125\n",
      "125/125 [==============================] - 17s 138ms/step - loss: 0.0100 - acc: 0.7928\n",
      "Epoch 115/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0097 - acc: 0.7929\n",
      "Epoch 116/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0097 - acc: 0.7929\n",
      "Epoch 117/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0098 - acc: 0.7929\n",
      "Epoch 118/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0102 - acc: 0.7929\n",
      "Epoch 119/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0097 - acc: 0.7929\n",
      "Epoch 120/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0095 - acc: 0.7930\n",
      "Epoch 121/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0096 - acc: 0.7929\n",
      "Epoch 122/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0095 - acc: 0.7930\n",
      "Epoch 123/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0095 - acc: 0.7930\n",
      "Epoch 124/125\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.0094 - acc: 0.7930\n",
      "Epoch 125/125\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.0096 - acc: 0.7930\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, history = train(model,'training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Saved Model\n",
    "The trained model is now loaded so that it can be evaluated and used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model_train_new.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "The reconstruction error of all\n",
    "pixel values I in frame t of the video sequence is taken as the Euclidean distance\n",
    "between the input frame and the reconstructed frame:\n",
    "\n",
    "**e(t) = ||x(t) âˆ’ fW (x(t))||2**\n",
    "\n",
    "where fW is the learned weights by the spatiotemporal model. We then\n",
    "compute the abnormality score sa(t) by scaling between 0 and 1. Subsequently,\n",
    "regularity score sr(t) can be simply derived by subtracting abnormality score\n",
    "from 1:\n",
    "\n",
    "**sa(t) = (e(t) âˆ’ e(t)min)/e(t)max**\n",
    "\n",
    "**sr(t) = 1 âˆ’ sa(t)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mean_squared_loss(x1,x2):\n",
    "    #Compute Euclidean Distance Loss between input frame(x1) and the reconstructed frame(x2) pixel values.\n",
    "    diff=x1-x2\n",
    "    a,b,c,d,e=diff.shape\n",
    "    #Number of samples is product of all the dimensions.\n",
    "    n_samples=a*b*c*d*e\n",
    "    #Square of distance\n",
    "    sq_diff=diff**2\n",
    "    #Sum of Square of distance(difference) between pixel values.\n",
    "    Sum=sq_diff.sum()\n",
    "    #Mean of Sum of Square of distance(difference) between pixel values(MSE).\n",
    "    mean_dist=Sum/n_samples\n",
    "    return mean_dist\n",
    "\n",
    "def detectAnomaly(X_test,model):\n",
    "    #Just to print if any anomalies of no.\n",
    "    flag = 0\n",
    "    #Set after proper evaluating the irregularity score with manual observation in videos by approximately calculating\n",
    "    #frame bunch number.\n",
    "    threshold = 0.1\n",
    "    mainnum = 1\n",
    "    #Check for irregularity in set of 5 consecutive frame bunches. Each bunch is a set of 10 consecutive frames of 227x227.\n",
    "    for i in range(0,len(X_test),5):\n",
    "        inter = X_test[i:i+5]\n",
    "        losslist = []\n",
    "        bunchnumlist = []\n",
    "        for number,bunch in enumerate(inter):\n",
    "            n_bunch=np.expand_dims(bunch,axis=0)\n",
    "            reconstructed_bunch=model.predict(n_bunch)\n",
    "            loss=mean_squared_loss(n_bunch,reconstructed_bunch)\n",
    "            losslist.append(loss)\n",
    "        #Calculating the irregularity score from the MSE loss of 5 consecutive frame bunches.\n",
    "        #If the score of any frame bunch is greater than threshold, Print anomaly found.\n",
    "        for n,l in enumerate(losslist):\n",
    "            score = (l-min(losslist))/max(losslist)\n",
    "#             print(score)\n",
    "            if score > threshold:\n",
    "                print(\"Anomalous bunch of frames at bunch number {}. Score of frame {} was higher.\".format(mainnum,n+1))\n",
    "                flag=1\n",
    "        mainnum = mainnum+1\n",
    "    if flag==1:\n",
    "        print(\"Anomaly found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1258\n",
      "(125, 227, 227, 10)\n",
      "(125, 227, 227, 10, 1)\n",
      "Anomalous bunch of frames at bunch number 1. Score of frame 1 was higher.\n",
      "Anomalous bunch of frames at bunch number 3. Score of frame 4 was higher.\n",
      "Anomalous bunch of frames at bunch number 3. Score of frame 5 was higher.\n",
      "Anomalous bunch of frames at bunch number 5. Score of frame 1 was higher.\n",
      "Anomalous bunch of frames at bunch number 5. Score of frame 2 was higher.\n",
      "Anomalous bunch of frames at bunch number 5. Score of frame 3 was higher.\n",
      "Anomalous bunch of frames at bunch number 8. Score of frame 5 was higher.\n",
      "Anomalous bunch of frames at bunch number 9. Score of frame 1 was higher.\n",
      "Anomalous bunch of frames at bunch number 9. Score of frame 2 was higher.\n",
      "Anomalous bunch of frames at bunch number 11. Score of frame 4 was higher.\n",
      "Anomalous bunch of frames at bunch number 11. Score of frame 5 was higher.\n",
      "Anomalous bunch of frames at bunch number 12. Score of frame 1 was higher.\n",
      "Anomalous bunch of frames at bunch number 12. Score of frame 2 was higher.\n",
      "Anomalous bunch of frames at bunch number 14. Score of frame 5 was higher.\n",
      "Anomalous bunch of frames at bunch number 15. Score of frame 1 was higher.\n",
      "Anomalous bunch of frames at bunch number 15. Score of frame 3 was higher.\n",
      "Anomalous bunch of frames at bunch number 16. Score of frame 1 was higher.\n",
      "Anomalous bunch of frames at bunch number 16. Score of frame 2 was higher.\n",
      "Anomalous bunch of frames at bunch number 18. Score of frame 3 was higher.\n",
      "Anomalous bunch of frames at bunch number 18. Score of frame 4 was higher.\n",
      "Anomalous bunch of frames at bunch number 18. Score of frame 5 was higher.\n",
      "Anomalous bunch of frames at bunch number 20. Score of frame 1 was higher.\n",
      "Anomalous bunch of frames at bunch number 21. Score of frame 2 was higher.\n",
      "Anomalous bunch of frames at bunch number 21. Score of frame 5 was higher.\n",
      "Anomalous bunch of frames at bunch number 24. Score of frame 1 was higher.\n",
      "Anomalous bunch of frames at bunch number 24. Score of frame 2 was higher.\n",
      "Anomalous bunch of frames at bunch number 25. Score of frame 5 was higher.\n",
      "Anomaly found\n"
     ]
    }
   ],
   "source": [
    "X_test,_ = loadFrames('testing.npy')\n",
    "detectAnomaly(X_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The model was first tested on the testing dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on a particular video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The model was now tested on a particular video to see if it has anomalies.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "(9, 227, 227, 10)\n",
      "(9, 227, 227, 10, 1)\n",
      "Anomalous bunch of frames at bunch number 1. Score of frame 2 was higher.\n",
      "Anomaly found\n"
     ]
    }
   ],
   "source": [
    "imstore = []\n",
    "preprocess(os.getcwd()+'/mytest/', imstore, 'mytesting',2)\n",
    "del imstore\n",
    "X_test,_ = loadFrames('mytesting.npy')\n",
    "detectAnomaly(X_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on live feed\n",
    "**Live camera feed is captured and checked for anomalies if any**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "from scipy.misc import imresize\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n",
      "Model loaded\n",
      "Anomalous bunch of frames at bunch number 1. Score of frame 4 was higher.\n",
      "Anomaly found\n"
     ]
    }
   ],
   "source": [
    "vc=cv2.VideoCapture(0)\n",
    "rval=True\n",
    "print('Loading model')\n",
    "model=load_model('model_train_new.h5')\n",
    "print('Model loaded')\n",
    "threshold = 0.1\n",
    "for k in range(10):\n",
    "    imagedump=[]\n",
    "    for j in range(10):\n",
    "        for i in range(10):\n",
    "            rval,frame=vc.read()\n",
    "            frame=imresize(frame,(227,227,3))\n",
    "            #Convert the Image to Grayscale\n",
    "            gray=0.2989*frame[:,:,0]+0.5870*frame[:,:,1]+0.1140*frame[:,:,2]\n",
    "            gray=(gray-gray.mean())/gray.std()\n",
    "            gray=np.clip(gray,0,1)\n",
    "            imagedump.append(gray)\n",
    "    imagedump=np.array(imagedump)\n",
    "    imagedump.resize(10,227,227,10)\n",
    "    imagedump=np.expand_dims(imagedump,axis=4)\n",
    "    print(imagedump.shape)\n",
    "    detectAnomaly(imagedump, model)\n",
    "    \n",
    "    \n",
    "#     output=model.predict(imagedump)\n",
    "#     loss=mean_squared_loss(imagedump,output)\n",
    "#     if loss>threshold:\n",
    "#         print('Anomalies Detected')\n",
    "        \n",
    "        \n",
    "#     for number,bunch in enumerate(inter):\n",
    "#         n_bunch=np.expand_dims(bunch,axis=0)\n",
    "#         reconstructed_bunch=model.predict(n_bunch)\n",
    "#         loss=mean_squared_loss(n_bunch,reconstructed_bunch)\n",
    "#         losslist.append(loss)\n",
    "#         bunchnumlist.append(number)\n",
    "#         #Calculating the irregularity score from the MSE loss of 5 consecutive frame bunches.\n",
    "#         #If the score of any frame bunch is greater than threshold, Print anomaly found.\n",
    "#     for n,l in enumerate(losslist):\n",
    "#         score = (l-min(losslist))/max(losslist)\n",
    "#         if score > threshold:\n",
    "#             print(\"Anomalous bunch of frames at bunch number {}. Score of frame {} was higher.\".format(mainnum,n+1))\n",
    "#             flag=1\n",
    "vc.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "**The model gave decent predictions on all the 3, Testing dataset, on a standalone video and on a live feed. The training data had maximum data of people walking far away from the camera and hence when someone walks near to the camera, the model shows Anomaly Found. The verification of the prediction of model on the Testing dataset was done by manually visiting the testing dataset frames and calculating the video from the frame bunch number and FramesPerSeconds used to extract frames from videos. Since its an unsupervised model, this was the only approach came to my mind to evaluate the model's testing accuracy. Hence apart from the frame numbers and bunch numbers, i could not display any other data to showcase the model accuracy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To conclude, A Spatiotemporal CNN can be used to train a model using unsupervised learning which can help in finding anomalies, which can be difficult to find by a supervised learning algorithm due to lack of unbiased datasets.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "https://www.semanticscholar.org/paper/An-Overview-of-Deep-Learning-Based-Methods-for-and-Kiran-Thomas/7198f45e979d4e7bb2ad2f8a5f098ab196c532b6\n",
    "\n",
    "https://www.semanticscholar.org/paper/Improved-anomaly-detection-in-surveillance-videos-a-Khaleghi-Moin/1a5c917ec7763c2ff9619e6f19d02d2f254d236a\n",
    "\n",
    "https://www.semanticscholar.org/paper/A-Short-Review-of-Deep-Learning-Methods-for-Group-Borja-Borja-Saval-Calvo/d9db8a4ce5ae4c4d03a55c648f4e7006838b6952\n",
    "\n",
    "https://www.semanticscholar.org/paper/Context-encoding-Variational-Autoencoder-for-Zimmerer-Kohl/2f3a2e24fb0ea3a9b6a4ebf0430886fdfa3efdd3\n",
    "\n",
    "https://machinelearningmastery.com/cnn-long-short-term-memory-networks/\n",
    "\n",
    "https://arxiv.org/abs/1411.4389\n",
    "\n",
    "https://www.coursera.org/lecture/nlp-sequence-models/long-short-term-memory-lstm-KXoay\n",
    "\n",
    "https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\n",
    "\n",
    "https://www.ncbi.nlm.nih.gov/pubmed/22392705\n",
    "\n",
    "https://arxiv.org/abs/1604.04574\n",
    "\n",
    "https://www.researchgate.net/publication/221361667_Anomaly_detection_in_extremely_crowded_scenes_using_spatio-temporal_motion_pattern_models\n",
    "\n",
    "https://pennstate.pure.elsevier.com/en/publications/adaptive-sparse-representations-for-video-anomaly-detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaration of adapted code\n",
    "* The approach for preprocessing and providing proper input to model was referenced from paper https://arxiv.org/pdf/1701.01546.pdf . **But the code was entirely written after understanding the approach.**\n",
    "* The model architecture was developed from paper https://arxiv.org/pdf/1701.01546.pdf . **But this was considered as the base model and many improvements were made over this model.**\n",
    "* The code for grayscaling a colored image was referenced from stackoverflow post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scope of this project\n",
    "Since this project was a solo project, and considering that this was my first time working with video analytics, I feel that understanding the video preprocessing and its feature extraction, performing experiments on the base model architecture (which can be viewed in experiments.ipynb), evaluating the model manually on the Testing dataset, on an individual standalone video and in live camera feed was sufficient enough for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIT License\n",
    "\n",
    "Copyright (c) 2019 Kirti Shukla\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
